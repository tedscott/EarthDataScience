---
title: "Twitter Sentiment Analysis - Text Mining Assignment"
output: html_notebook
---

# Playing around with rtweet package and looking into sentiment of tweets about topics



### get libraries we need

```{r}
# https://docs.ropensci.org/rtweet/index.html
#install.packages("rtweet")

# https://github.com/EmilHvitfeldt/textdata
#install.packages("textdata")

# https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
#install.packages("tidytext")
#install.packages("leaflet")

library(rtweet)
library(tidyverse)
library(tidytext)
library(textdata)
library(httpuv) # needed for twitter auth
library(leaflet) #interactive mapping

```

## search twitter for some terms
### search formatting details: https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators


```{r}
# for now, use my personal twitter creds
auth_setup_default()

# first time it prompted to add another package httpuv
# and also launches a webpage to authenticate

# try a search - conditional format at link above
searchterms <- "Biden AND climate"
# for n=10000 took a couple of minutes, let's time it to be sure
start1 <- Sys.time()        # Save starting time
tw <- search_tweets(searchterms,n = 10000, include_rts = F)
end1 <- Sys.time()          # Finishing time
(time1 <- end1 - start1) # total time
```


## Look at the raw data 
### can take a while...

```{r}
# take a look at the data
head(tw)

# what are those coordinates data objects - seems complex
head(tw$coordinates)

# rtweet provides a helper function lat_lng()
# https://docs.ropensci.org/rtweet/reference/lat_lng.html
locs <- lat_lng(tw)
head(locs,30)
sum(!is.na(locs$lat)) # many are NA but some are not


```


## time to remove things we don't want to analyze and clean up the remaining stuff

```{r}
# 43 columns, most of which are not useful, so reduce it down 
# to just a few potentially useful ones
# coordinates are a tbl_df so can't use dplyr select

filtered_tw <- data.frame(date_time = locs$created_at,
                          text = locs$text,
                          lat = locs$lat,
                          long = locs$lng)
# filtered_tw_noNA <- filtered_tw %>% filter(!is.na(lat))
# head(filtered_tw_noNA)

# format date_time and remove links from text with regex
cleaned_tw <- filtered_tw %>%
  mutate(date_time = as.POSIXct(
           date_time, format="%a %b %d %H:%M:%S +0000 %Y"),
         cleaned_text = gsub("http.*","", filtered_tw$text)) %>%
  mutate(cleaned_text = gsub("https.*","", filtered_tw$text))


# remove any links
# filtered_tw$cleaned_text <- gsub("http.*","", filtered_tw$text)
# filtered_tw$cleaned_text <- gsub("https.*","", filtered_tw$text)
head(cleaned_tw,10)

```


# where are these tweets coming from?

```{r}
# make a basemap
world_basemap <- ggplot() +
  borders("world", colour="gray85", fill="gray80")

world_basemap

# remove rows with no location information
cleaned_tw_locs <- cleaned_tw %>% na.omit()
head(cleaned_tw_locs)

# plot them on the map
world_basemap +
  geom_point(data=cleaned_tw_locs, aes(x=long, y=lat), 
             col = "purple", alpha=0.5) +
  labs(title = "Tweet locations for Biden AND Climate")

```

# make it interactive to explore tweets from those locations
```{r}

site_locations <- leaflet(cleaned_tw_locs) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~long, lat = ~lat, popup = ~cleaned_text,
                   radius = 3, stroke = F)

site_locations

```

# OK, that was fun but now let's look at the words in the tweets


```{r}
# back to full dataset of tweets regardless of location data
# convert all words to lower case and also remove punctuation using unnest_tokens()
# this splits all tweets into individual words
head(cleaned_tw)
glimpse(cleaned_tw)
summary(cleaned_tw)
str(cleaned_tw)
updated_tw <- cleaned_tw %>% 
  dplyr::select(cleaned_text) %>%  # need dplyr:: since select is overloaded
  unnest_tokens(word, cleaned_text)

head(updated_tw,30)


# what are the top words?
updated_tw %>%
  count(word, sort=T) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x=word, y=n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y="Count")


# what do you notice about the words?
# need to remove "stop words"
     
```



## Remove "stop words" (common filler words)

```{r}
# time to remove stop words
# load them into memory in tidytext format
data("stop_words")
head(stop_words,20)

# how many words in our dataset?
prefiltered <- nrow(updated_tw)

# use anti_join to remove the stop words
# anti_join(x,y) return all rows from x without a match in y
updated_tw <- anti_join(updated_tw,stop_words)

# how many did we lose?
postfiltered <- nrow(updated_tw)
# roughly half removed
pctremoved <- (prefiltered - postfiltered)/prefiltered*100
pctremoved

# let's check the sentiment of the remaining words
# using the dictionary "bing" (not Microsoft)
sentiment_tw <-  updated_tw %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = T) %>%
  ungroup()

# check the top few
head(sentiment_tw, 20)
```


# Graph the top positive and negative words from those tweets

```{r}
# graph em
# plot the negatives and positives
sentiment_tw %>%
  group_by(sentiment) %>%
  top_n(20) %>%
  ungroup() %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word,n,fill = sentiment)) +
  geom_col(show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = paste("Tweets containing",searchterms), y = NULL, x = NULL) +
  coord_flip() +
  theme_classic()
```


# What's the overall proportion of positive to negative terms in the tweets?

```{r}
# column plot of proportion of positive vs negative %
sentiment_tw %>%
  count(sentiment) %>%
  mutate(pct = n / sum(n)) %>%
  ggplot(aes(x = factor(sentiment), fill = factor(sentiment))) + 
    geom_col(aes(y = pct)) +
    labs(title = "Percent of each sentiment in the words", y = NULL, x = NULL) +
    scale_y_continuous(labels = scales::percent) # switch from numeric to percent


```

## interestingly, when I ran this the sentiment was overwhelmingly negative
## some of this is because the words around climate change ARE negative: crisis, emergency, etc
## but recent polling (prior to any passage of the "Inflation Reduction Act of 2022")
## suggests the majority of the USA are in favor, by a large margin

![an image caption Source: Data For Progress Newsletter 7/29/2022.](images/DFP_poll_climate_legislation_april2022.jpg)


